{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LaTok\n",
    "\n",
    "## Linear Algebraic Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Idea and Motivation\n",
    "\n",
    "* Tokenization performance is critical\n",
    "    * Many NLP tasks require tokenization of large corpora\n",
    "        * Poor tokenization performance is a processing bottleneck\n",
    "    * There are a lot of tokenizers with varying\n",
    "        * behavior\n",
    "        * configurability\n",
    "        * performance\n",
    "* NumPy is fast\n",
    "    * In practice, existing NumPy functions often outperform raw python implementations and often even Cython optimizations.\n",
    "\n",
    "\n",
    "### Question: Could numpy.split be a basis for a fast, configurable tokenizer?\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing using NumPy Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a **text** string, mark the positions of \"split\" points with a **mask**\n",
    "\n",
    "For example, to split on spaces, align mask 1's with the text's spaces:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'This is a test'\n",
    "mask = '00001001010000'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turn these into alignable numpy arrays suitable for applying the \"split\" function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('T', 0), ('h', 0), ('i', 0), ('s', 0), (' ', 1), ('i', 0), ('s', 0), (' ', 1), ('a', 0), (' ', 1), ('t', 0), ('e', 0), ('s', 0), ('t', 0)]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "ntext = np.array(list(text))\n",
    "nmask = np.array(list(mask), dtype=np.int8)\n",
    "print(list(zip(ntext, nmask)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See what the split looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['T', 'h', 'i', 's'], dtype='<U1'),\n",
       " array([' ', 'i', 's'], dtype='<U1'),\n",
       " array([' ', 'a'], dtype='<U1'),\n",
       " array([' ', 't', 'e', 's', 't'], dtype='<U1')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.split(ntext, np.flatnonzero(nmask))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And convert back to tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This', 'is', 'a', 'test']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[''.join(a).strip() for a in np.split(ntext, np.flatnonzero(nmask))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hey, this might be crazy enough to actually work!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization via Linear Operations on a Feature Matrix\n",
    "\n",
    "1. Define $\\mathbf{m}$ character features\n",
    "    * e.g., isspace, isalpha, etc.\n",
    "2. Vectorize the $\\mathbf{n}$ text characters into an $\\mathbf{n}$x$\\mathbf{m}$ feature matrix, $\\mathbf{F}$, where each element $\\mathbf{f_{i,j}}$ is the $\\mathbf{j^{th}}$ feature for the $\\mathbf{i^{th}}$ character.\n",
    "    * One row for each text character\n",
    "    * Each column represents a character feature\n",
    "\n",
    "$$ F = \\begin{bmatrix} f_{1,1} & f_{1,2} & \\ldots & f_{2,m} \\\\ f_{2,1} & f_{2,2} & \\ldots & f_{2,m} \\\\ \\ldots & \\ldots & \\ldots & \\ldots \\\\ f_{n,1} & f_{n,2} & \\ldots & f_{n,m} \\end{bmatrix} $$\n",
    "\n",
    "3. Define a linear transformation, $\\mathbf{S}$, on $\\mathbf{F}$ to produce an $\\mathbf{n}$-dimensional split mask vector, $\\mathbf{s}$,\n",
    "    * Where vector values are non-zero at character positions at which to split and consecutively zero to identify spans of characters to be combined as tokens.\n",
    "\n",
    "$$ \\begin{equation*} \\mathbf{S} ( \\mathbf{F} ) = \\mathbf{s} \\end{equation*} $$\n",
    "\n",
    "so that\n",
    "\n",
    "$$ s = \\begin{bmatrix} s_{1} \\\\ s_{2} \\\\ \\ldots \\\\ s_{n} \\end{bmatrix} $$\n",
    "\n",
    "where\n",
    "\n",
    "$$ \\begin{align} \\mathbf{s_{i}} \\neq 0, \\quad \\quad \\text{to start a token at position i}, \\\\ \\text{and} \\quad \\mathbf{s_{j}} = 0, \\quad \\quad \\text{for characters within tokens} \\end{align} $$\n",
    "\n",
    "4. Apply the split mask to tokenize the text string\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...Next, let's pursue this idea with a concrete example..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
