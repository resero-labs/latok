{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer usage\n",
    "\n",
    "A notebook for experimenting with and understanding tokenization and its options.\n",
    "\n",
    "First, some initializations..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import latok.core.default_tokenizer as tokenizer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters\n",
    "\n",
    "**Edit the following parameters as desired**, including the text to tokenize, before running the next cell..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''This is a1 test don't http://foo.com?bar=123 @user abc@xyz.com camelCaseOne, CamelCaseTwo, camelCase1, CamelCase2, 123 $123,456.78'''\n",
    "\n",
    "\n",
    "# Abstract features:\n",
    "#  A list of FeatureSpec instances to use for abstract featurization\n",
    "#  (Refer to \"abstract featurization\" description below)\n",
    "feature_specs = [\n",
    "    tokenizer.TWITTER_FEATURE,\n",
    "    tokenizer.EMAIL_FEATURE,\n",
    "    tokenizer.URL_FEATURE,\n",
    "    tokenizer.CAMEL_CASE_FEATURE,\n",
    "    tokenizer.NUMERIC_FEATURE,\n",
    "    tokenizer.EMBEDDED_APOS_FEATURE,\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Tokenization\n",
    "\n",
    "tokenizer.tokenize generates each token.\n",
    "\n",
    "We'll wrap each token for placement in a pandas DataFrame for display..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_texts = [(token_text) for token_text in tokenizer.tokenize(text)]\n",
    "\n",
    "print(f'Basic tokenization of text:\\n\\n{text}\\n')\n",
    "\n",
    "df = pd.DataFrame(token_texts, columns=['token'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LaToken object generation\n",
    "\n",
    "Instead of just token text, a LaToken object can be generated for each token using the ```featurize``` method.\n",
    "\n",
    "This, among other things, preserves the original character locations of the tokens.\n",
    "\n",
    "First, build and populate the token objects to be displayed below..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "la_tokens = list(tokenizer.featurize(text))\n",
    "tokenizer.add_abstract_features(la_tokens, feature_specs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the basic objects..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Object tokenization of text:\\n\\n{text}\\n')\n",
    "\n",
    "data = [(token.text, token.start_idx, token.end_idx) for token in la_tokens]\n",
    "df = pd.DataFrame(data, columns=['token', 'start_idx', 'end_idx'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Featurization\n",
    "\n",
    "For many use cases, the text of tokens is sufficient for further processing; for others, capturing features for each token is desired.\n",
    "\n",
    "* Character-level features are directly available from the feature matrix.\n",
    "* Token features can be expressed\n",
    "    * **_directly_** as the sum of all character features for the token\n",
    "        * These amount to \"characteristic\" vectorization of tokens, where vectors for all tokens having the same combination of character features are equivalent.\n",
    "    * **_abstractly_** as the labeled combination of multiple character features\n",
    "        * **NOTE:** These are typically labels given to the offset combinations used to split text into tokens in the first place, but can include the use of other rules, like regular expression (in)validation, over the token data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Direct token featurization\n",
    "\n",
    "Show the direct characteristic vector for the featurized tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Featurized tokenization of text:\\n\\n{text}\\n')\n",
    "\n",
    "data2 = [(token.text,\n",
    "          token.start_idx, token.end_idx,\n",
    "          ' '.join(str(f) for f in token.features))\n",
    "         for token in la_tokens]\n",
    "df = pd.DataFrame(data2, columns=['token', 'start_idx', 'end_idx', 'characteristic_vector'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Abstract token featurization\n",
    "\n",
    "Show the abstract feature specs to the token objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Abstract featurization of text:\\n\\n{text}\\n')\n",
    "\n",
    "data3 = [(token.text,\n",
    "          token.start_idx, token.end_idx,\n",
    "          token.abstract_features)\n",
    "         for token in la_tokens]\n",
    "df = pd.DataFrame(data3, columns=['token', 'start_idx', 'end_idx', 'abstract_features'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
