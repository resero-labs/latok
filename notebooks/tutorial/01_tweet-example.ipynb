{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Definition and Configuration for Tweet Tokenization\n",
    "\n",
    "Let's define tweet tokenization as follows:\n",
    "\n",
    "* Separate tokens on\n",
    "    * Whitespace\n",
    "    * Symbols, except as described below\n",
    "        * Where each symbol becomes its own token\n",
    "    * CamelCase, except for special cases described below\n",
    "* Keep twitter \"special\" character prefixes with tokens\n",
    "    * User (@, .@), Hashtag (#), Signature (^), Cashtag ($)\n",
    "    * Don't split camelCasing in \"special\" twitter tokens\n",
    "* Keep each url as a token\n",
    "* Keep each email address as a token\n",
    "* Keep embedded apostrophes in a token\n",
    "    * e.g., \"don't\", \"isn't\", \"can't\", \"John's\", etc.\n",
    "* Keep all non-white spans of characters with a digit together in a token\n",
    "    * Except for trailing punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:\n",
    "\n",
    "\"@Mary, check out John's AmazingMuscleCar for $100K! at http://johnscar.com opinions@research.com! #LiveTheDream\"\n",
    "\n",
    "Tokenizes to:\n",
    "* @Mary\n",
    "* ,\n",
    "* check\n",
    "* out\n",
    "* John's\n",
    "* Amazing\n",
    "* Muscle\n",
    "* Car\n",
    "* for\n",
    "* $100K\n",
    "* !\n",
    "* at\n",
    "* http://johnscar.com\n",
    "* opinions@research.com\n",
    "* !\n",
    "* #LiveTheDream"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we need to define features and transformations for these rules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Point Rule Transformations\n",
    "\n",
    "First, we can define features for identifying single split points in a string and corresponding masks for the transformation:\n",
    "\n",
    "|Rule|Features|Description|Mask|\n",
    "|---|---|---|---|\n",
    "|Split on whitespace|Space|char.isspace()|Space|\n",
    "|Split on symbols|Symbol|non-space, non-alphanum|Symbol|\n",
    "|Split on CamelCase|Upper, Lower, NextLower, PrevLower|split at upper following lower or at upper followed by lower|(Upper & PrevLower) \\| (Upper & NextLower)| \n",
    "\n",
    "The transformation for these rules is to generate a mask that \"or\"'s features to split on across all rules, where each rule's features may be \"and\"ed.\n",
    "\n",
    "For example, note how the CamelCase rule \"and\"'s the character feature combinations of \"Upper & PrevLower\" and \"Upper & NextLower\".\n",
    "\n",
    "With the combined transformation mask for the point rules of:\n",
    "\n",
    "$$ \\begin{equation*} \\mathbf{PointTransforms} = (Space + Symbol + (Upper * PrevLower) + (Upper * NextLower)) \\end{equation*} $$\n",
    "\n",
    "Note that the logical \"and\" and \"or\" operations correspond to the mathematical \"\\*\" and \"+\" operations, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Block Rule Transformations\n",
    "\n",
    "For the rules that pertain to keeping groups of characters together, even though they would be split apart by other rules (for example, splitting on symbols would break apart a url, email, and twitter special tokens), we define \"block\" masks that mask a block of consecutive characters as \"0\"'s, or \"don't split\", to be anded with the other masks, hence preventing a split from within the block spans.\n",
    "\n",
    "For these, we define the $\\mathbf{block\\_mask(locator\\_mask, endpoints\\_mask)}$ function that generates a mask of $\\mathbf{10\\ldots0}$ between endpoints within a span that the locator is present.\n",
    "\n",
    "|Rule|Features|Description|Mask|\n",
    "|---|---|---|---|\n",
    "|Keep twitter specials together|Twitter, PrevSpace, NextAlpha, ., Next_@, AfterNextAlpha|Twitter special char following a space character and preceding an alphabetical character or matching the \" .@a\" pattern, where \"a\" is any alpha.|$\\mathbf{block\\_mask}$((Twitter & PrevSpace & NextAlpha) \\| (. & PrevSpace & Next_@ & AfterNextAlpha), Space)|\n",
    "|Keep urls as a single token|:, Next_/, AfterNext_/, PrevAlpha, Space|Locate a url by finding \"a://\", where \"a\" is any alpha and span from preceding to subsequent space|$\\mathbf{block\\_mask}$(: & Next_/ & AfterNext_/ & PrevAlpha, Space)|\n",
    "|Keep email addresses as a single token|@, PrevAlpha, NextAlpha, Space|Locate an email address by finding an atset (@) embedded between two alphas.|$\\mathbf{block\\_mask}$(@ & PrevAlpha & NextAlpha, Space)|\n",
    "|Keep embedded apostrophes within tokens|Apos, PrevAlpha, NextAlpha|Locate an embedded apostrophe surrounded by alphas|$\\mathbf{block\\_mask}$(Apos & PrevAlpha & NextAlpha, Space)|\n",
    "|Keep tokens with digits together|Numeric, Space|Keep tokens with a numeric character together|$\\mathbf{block\\_mask}$(Numeric,Space)|\n",
    "\n",
    "Where the \"Twitter\" feature is a custom defined character feature that is true for the characters: @, #, $, and ^.\n",
    "\n",
    "Because all block masks operate between Space features, the rules can be combined into a single transformation and the block_mask function can be applied later to the combination:\n",
    "\n",
    "$$ \\begin{equation*} \\mathbf{BlockTransforms} = ((Twitter * PrevSpace * NextAlpha) * (. * PrevSpace * Next\\_@ * AfterNextAlpha)) \\\\ + (: * Next\\_/ * AfterNext\\_/ * PrevAlpha) + (@ * PrevAlpha * NextAlpha) \\\\ + (Apos * PrevAlpha * NextAlpha) + (Numeric) \\end{equation*} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post-Block Transform\n",
    "\n",
    "One more point transformation is needed after the block transformations to satisfy the rule to split off any trailing token symbol from tokens:\n",
    "\n",
    "$$ \\begin{equation*} \\mathbf{EndSymbolTransform} = (Symbol + NextSpace) \\end{equation*} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combined and applied transformations\n",
    "\n",
    "Up to this point, the aforementioned transformations can be defined and precomputed for application to the feature matrix of any input string for tokenization.\n",
    "\n",
    "Next, for each input feature matrix, $\\mathbf{F}$, the transformations need to be applied. Because of the insertion of the $\\mathbf{block\\_mask}$ function wrapping the block transforms, these will need to be applied separately.\n",
    "\n",
    "The application function is defined as:\n",
    "\n",
    "$$ \\mathbf{S}(\\mathbf{F}) = \\mathbf{apply\\_transform}(\\mathbf{F}, \\mathbf{T}) $$\n",
    "\n",
    "For any feature matrix, $\\mathbf{F}$, and transform, $\\mathbf{T}$, to generate a split mask vector based on the feature matrix, $\\mathbf{F}$.\n",
    "\n",
    "Combining the point, block, and end symbol transformations, we get the final transformation function:\n",
    "\n",
    "$$ \\begin{equation*} \\mathbf{S}(\\mathbf{F}) = (\\mathbf{apply\\_transform}(\\mathbf{F}, PointTransforms) \\\\ * \\\\ \\mathbf{apply\\_transform}(\\mathbf{F}, \\mathbf{block\\_mask}(BlockTransforms, Space))) \\\\ + \\\\ \\mathbf{apply\\_transform}(\\mathbf{F}, EndSymbolTransform) \\end{equation*} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LaTok Implementation Details\n",
    "\n",
    "1. Collected and implement character features (unicode c-extension)\n",
    "    * Leverage python unicode feature implementation, customizing with new features\n",
    "1. Implemented necessary new helper functions (numpy c-extensions):\n",
    "    * feature matrix construction\n",
    "        * building the feature matrix as a NumPy array in python ended up being a choke point\n",
    "        * a custom c-extension to build this matrix showed vast improvement\n",
    "    * apply_transform\n",
    "        * custom c implementation of transformation application to the feature matrix improved performance\n",
    "    * block_mask\n",
    "        * custom c implementation of generating the block mask also improved performance\n",
    "1. Implemented LaTok algorithm as a general tokenizer (python ```tokenize```)\n",
    "1. Defined transformation rules for twitter case (python ```gen_split_mask```)\n",
    "1. Executed the tokenizer (python)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Configuration Details\n",
    "\n",
    "With the basic tokenizer implemented with a sample default \"tweet\" configuration, alternate tokenization use cases can be implemented.\n",
    "\n",
    "1. Collect character features, implementing any that are new (unicode c-extension)\n",
    "1. Implement any new helper functions (numpy c-extensions)\n",
    "    * In particular, new types of masking operations may be necessary\n",
    "1. Define transformation rules, implementing a custom ```gen_split_mask```\n",
    "1. Execute ```tokenize``` with the custom ```gen_split_mask``` function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A view of the feature matrix and split vectors\n",
    "\n",
    "Revisiting our sample tweet from above..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from latok.core.latok_utils import gen_parse_matrix, FEATURE_NAMES\n",
    "from latok.core.default_tokenizer import gen_split_mask, tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"@Mary, check out John's AmazingMuscleCar for $100K! at http://johnscar.com opinions@research.com! #LiveTheDream\"\n",
    "\n",
    "m = gen_parse_matrix(text)\n",
    "df = pd.DataFrame(m, columns=FEATURE_NAMES)\n",
    "\n",
    "chars = pd.Series([c for c in text])\n",
    "splits = pd.Series(gen_split_mask(m))\n",
    "\n",
    "df = pd.concat((chars.rename(\"Chars\"), splits.rename(\"Splits\"), df), axis=1)\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_rows = None\n",
    "\n",
    "print(f'Tokens:\\n{list(tokenize(text))}\\n\\nFeatures:')\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
